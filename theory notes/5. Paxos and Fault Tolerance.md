```mermaid
flowchart TD
  init([Init])-->|Send prep|wpp(Wait for f+1)
  wpp-->|at least f+1|p(Prepared)
  p-->|Send commit|wp(Wait for f+1)
  wp-->|at least f+1|c(Committed)
  c-->fin([Complete])
  wpp-->to([Timeout])
  wp-->to
  to-->|restart|init
  
```
### Recoverability
- Atomicity (all-or-nothing) for updates involving a single server
- Atomicity (all-or-nothing) for updates involving more than one server (2 Phase Commit)
- **Availability and fault tolerance through replication (our focus)**
### Replicated State Machine Rules
- Rule #1: All replicas start in the same initial state
- Rule #2: Every replica apply **write operations in the same order**
- Rule #3: All operations must be deterministic
### RSM Failure Scenarios
- Primary crash (who will take over?)
- Multiple Primary (there can only be one or they will be inconsistency)
- Primary crash before sending all values, inconsistent replicas

## Paxos: Fault Tolerant Consensus
Machines agree despite:
- node failure
- network failure
- network delay/degradation

They mainly agree in 2 ways:
- Agree "X" node is the primary (election can be done using Paxos itself)
- "W" operation is the most recent operation to be executed.

What does agreement mean? How does it happen? (correctness)
- All nodes agree eventually on the same value
- Agree value must be proposed by some node

Fault Tolerance
-  If less than a fraction (N/2) of nodes fail, the rest should still reach agreement.
	- FLP theorem: impossible for a set of nodes in async system to agree on binary value, even if only 1 machine fails unexpectedly
	- Most networks are async.
	- So how can Paxos guarantee to reach consensus even in the presence of failures?
	- But Paxos doesnt guarantee the consensus process will terminate
	- This means consensus can just keep retrying until the node returns.

Node Roles
- Leader
- Acceptor (quorum)
- Learner

### Paxos Challenges
- 2 machines choose to be leader and solicit different values
- Leader crash before solicit
- Network partition: leader does not get majority (fault)
- leader crash after deciding but before announcing (re elect?)
- new leader decides a new value after the decision by a leader that crash
	- kinda similar to first challenge
	- inconsistency.
## Paxos Solution

Order proposals by ID such as Machine ID #: Logical Clock

Each acceptor can accept multiple proposals, but reach eventual consensus by all higher proposal ID following the same value.
- does previously accepted proposal get rescinded?
- no it just will get "ignored" at the proposer when it receives prepare-ok for higher proposal numbers

Paxos Node State:
- Na: highest proposal ID# accepted
- Va: Accepted value for Na
- Nh: Highest proposal ID# seen
	- seen is not the same as accept
- myn: my proposal ID# in the current Paxos session
	- when does it change.

### Three stage protocol
- Prepare: One or more proposer sends a message that it is preparing to become a leader and acceptors accept/reject proposals
	- a form of temporary election?
- Accept: Proposers that passes the prepare stage proposes values to acceptors and acceptors accept/rejects proposal values
- Decide: Proposer that passes the accept stage decides the final value and announces to learners

Prepare stage
potential proposer asks some nodes. they can reply:
- (prepare-ok, Na, Va): that was such a big number. im going to update my highest seen number (Nh)
	- why do we send Na, Va?
- (prepare-reject:) nah i seen a bigger one.
- depends on whether the other node has seen a higher number
- represents a promise that the acceptor will not respond to any other proposal less than myn.

Accept Stage.(proposer: plz accept plz)
Proposer previously did not include a value, but if it receives (prepare-ok) from a majority it can decide a value. it chooses value of the highest proposal ID it saw.

- send (accept, myn, Va) to those who replied with (prepare-ok)
- acceptors reply (accept-ok) if proposal ID is bigger than what they've seen (same as before)
- if not they reply (accept-reject)

Decide Stage
- After receiving accept-ok from a majority of acceptors, the proposer decides the value and sends to all learners.
- A value V is considered chosen at proposal number N iff majority of acceptors accepted in during accept stage of Proposal N.
- Once majority accepts value V, all future majorities will accept it too.
	- how?


### Paxos Properties
1. Proposal numbers are unique
2. Any 2 sets of acceptors have at least 1 common acceptor. Why?
3. The value sent out in the accept stage is the value of the highest number proposal ID# of all the responses in the prepare stage
PAD
prepare
accept
decide.

Timeout: All nodes wait for messages they expect and restart Paxos session starting from prepare on timeout.

Challenge 1: More than one leader/proposer
- can be due to network partition, congestion (timeout/ dropped packets)
- they will both start the prepare stage, so the different scenarios come in the behavior for later stages
- anyway its the later stages that determine consensus conflict

Anyway, there can be 2 leaders, with different proposal ID. N and N + 1 (because one is bigger than the other and helps to break tie/ achieve consensus)

Scenario 1: Proposer of N did not receive (accept-ok) from a majority of acceptors

the acceptors will not send (accept-ok) for (prepare, N) after seeing (prepare, N+1). they might send the first prepare-ok, but they wont send the accept-ok as they all updated to N+1 highest seen proposal ID#

so proposal ID N wasn't decided, no issues with consensus. (stopped in accept stage)

Scenario 2: Proposer of N did receive (accept-ok) from a majority.

Proposer of N tries to decide V. Since accept goes through, the acceptors didnt see the prepare message yet. 
So its okay, even though it seems like there is 2 leaders, the paxos decision making process was sequentially ordered and in consensus.


(Scenario 2): Leader of “N” did receive <accept-OK> from a majority of acceptors
leaderN+1 must receive <prepare-OK, Na, Va> from at least one node which has seen the value chosen by leaderN and therefore, leaderN+1 is aware of the value chosen by leaderN. 
Therefore, leaderN+1 will not choose a new value instead will use the value chosen by leaderN   (check the protocol for the “Accept” stage)

Consequence: leaderN  and leaderN+1 will reach agreement on the value

Caveat: You kind of need to restart Paxos to clear the Na Va values if u want to decide on a new value (e.g. a new leader election)


Challenge 2: What happens in Paxos when the leader with proposal ID# N crashes before soliciting to any other machines?
someone will timeout and become leader, choosing a higher proposal ID# than N ( automatic as N is based on clock also)

Challenge 3: Leader fails after sending a minority of accept messages
Timeout also, no consensus achieved.

Challenge 4: Leader fails after sending a majority of accept messages
accept ok goes ignored, timeout. new leader uses own Va when making a new proposal.

Challenge 5: What if there is a network partition and the leader does not get majority responses?

well it depends what the partitions looks like.
- If the leader is cut off in the minority, it will never get enough votes. new leader in majority partition will emerge.
- if leader is in majority, he will probably get enough votes.

Challenge 6: What if Acceptor fails after sending accept-OK. Thus, Phase 3 does not complete, as Node 3 times out after sending 
<decide, V> message to Node 4


- If permanent (or just longer than timeout), decide fails and paxos restarts
- If acceptor reboots, node 4 needs to recover Na  and Va from persistent storage (known from accept stage)
- Acceptor can become new leader. The new leader must reach agreement on the same value, choosing the same value as agreed before ensures the consensus (this is ensured by fetching Va from the disk)

